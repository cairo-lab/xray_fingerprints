{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6dbd19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import timm \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import efficientnet.keras as efn\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import transforms, models, datasets\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "METADATA_SUBSET_PATH = \"/Users/franceskoback/Documents/research/pytorch_1/metadata_100subset_df.csv\"\n",
    "\n",
    "def get_manufacturer_labels(encoder, target_variable = \"(0008, 0070) Manufacturer\"):\n",
    "    df = pd.read_csv(METADATA_SUBSET_PATH)\n",
    "    df[\"id\"] = df[\"id\"].astype(\"str\").str.zfill(8)\n",
    "    df[\"code\"] = encoder.fit_transform(df[target_variable])\n",
    "    \n",
    "    return {row[\"id\"]: row[\"code\"] for i, row in df.iterrows()}\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.img_dir = \"/Users/franceskoback/Documents/research/pytorch_1/xray_subsets\"\n",
    "\n",
    "        self.images = glob.glob(os.path.join(self.img_dir, \"*.npy\")) \n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        self.label_map = get_manufacturer_labels(self.le)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.fromarray(np.load(img_path)).convert(\"RGB\")\n",
    "        image = transforms.ToTensor()(image)\n",
    "        xray_id = os.path.basename(img_path).replace(\".npy\", \"\")\n",
    "        \n",
    "        return {\"image\": image, \"label\": self.label_map[xray_id]}\n",
    "\n",
    "def train_val_test_dataset(dataset, val_split=0.20):\n",
    "    train_idx, rem_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "\n",
    "    Test_size=0.5 # split equally between validataion and test sets \n",
    "\n",
    "    val_idx, test_idx  = train_test_split(list(range(len(rem_idx))), test_size=Test_size)\n",
    "\n",
    "\n",
    "\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['val'] = Subset(dataset, val_idx)\n",
    "    datasets['test'] = Subset(dataset, test_idx)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7225df93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "Training length 80\n",
      "Validation length 10\n",
      "Testing length 10\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomImageDataset()\n",
    "datasets = train_val_test_dataset(dataset)\n",
    "print(len(datasets['train'].dataset)) #6\n",
    "#datasets['train'].dataset, batch_size=params[\"batch_size\"], shuffle=True\n",
    "train_loader = DataLoader(\n",
    "    datasets['train'].dataset, batch_size=3, shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    datasets['val'].dataset, batch_size=3, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datasets['test'].dataset, batch_size=3, shuffle=True\n",
    ")\n",
    "print(len(train_loader.dataset)) #6 \n",
    "len_train=len(datasets['train'])\n",
    "len_val= len(datasets['val'])\n",
    "len_test= len(datasets['test'])\n",
    "print(\"Training length\", len(datasets['train']))\n",
    "print(\"Validation length\", len(datasets['val']))\n",
    "print(\"Testing length\", len(datasets['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80723e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franceskoback/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/franceskoback/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "def Net(num_classes):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Freeze parameters so we don't backprop through them\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    from collections import OrderedDict\n",
    "    classifier = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(2048, 1024)),\n",
    "                              ('relu', nn.ReLU()),\n",
    "                              ('fc2', nn.Linear(1024, 256)),\n",
    "                              ('relu', nn.ReLU()),\n",
    "                              ('fc3', nn.Linear(256, num_classes)),\n",
    "                              ('output', nn.LogSoftmax(dim=1))\n",
    "                              ]))\n",
    "\n",
    "    model.fc = classifier\n",
    "    return model\n",
    "\n",
    "params = {\n",
    "    \"model\": \"resnet50\",\n",
    "    #\"device\": \"cuda\",\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 3, #64\n",
    "    \"num_workers\": 1, #20\n",
    "    \"n_epochs\": 50, #100\n",
    "    \"image_size\": 224, \n",
    "    \"in_channels\": 3, #3\n",
    "    \"num_classes\": 3, #12\n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "model = Net(params['num_classes'])\n",
    "model.to(params[\"device\"])\n",
    "#loss_fn = nn.NLLLoss() \n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dee2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device = \"cpu\"):\n",
    "    #put model in training state\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "\n",
    "    for batch_idx, img_dicts in enumerate(train_loader,0):  \n",
    "        inputs = img_dicts[\"image\"] \n",
    "        labels = img_dicts[\"label\"]  \n",
    "        \n",
    "        inputs = Variable(inputs.to(device).float())\n",
    "        labels = Variable(labels.to(device).float())\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() # sets all grads to None \n",
    "\n",
    "        # print statistics\n",
    "        #running_loss += loss.item()\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "         #   print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            #running_loss=0\n",
    "          #  i=i+1\n",
    "\n",
    "        \n",
    "        \n",
    "        #\n",
    "    \n",
    "        train_loss+= ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n",
    "        if batch_idx%5==0:\n",
    "            print('train loss', train_loss)\n",
    "    \n",
    "    print('Epoch {} avg Training loss: {:.3f}'.format(epoch+1, train_loss))\n",
    "    \n",
    "    return model, train_loss\n",
    "\n",
    "def test_one_epoch(epoch, model, loss_fn, loader, len_val, device = \"cpu\"):\n",
    "    model.eval()\n",
    "    \n",
    "    #pbar = tqdm(enumerate(test_loader), total = len(test_loader))\n",
    "    running_loss = 0\n",
    "    actual_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    #for step, (imgs, labels) in pbar:\n",
    "    for batch_idx, img_dicts in enumerate(loader,0):    \n",
    "        inputs = img_dicts[\"image\"] \n",
    "        labels = img_dicts[\"label\"] \n",
    "        \n",
    "        inputs = Variable(inputs.to(device).float())\n",
    "        labels = Variable(labels.to(device).float())\n",
    "        \n",
    "        log_preds = model(inputs)\n",
    "        loss = loss_fn(log_preds, labels)\n",
    "        \n",
    "        preds = torch.exp(log_preds)\n",
    "        running_loss+=((1 / (batch_idx + 1)) * (loss.data.item() - running_loss))\n",
    "        \n",
    "        #calculate accuracy\n",
    "        top_prob, top_class = preds.topk(1, dim=1)\n",
    "        pred_labels+= list((top_class.view(-1)).cpu().numpy())\n",
    "        actual_labels+= list(labels.cpu().numpy())\n",
    "        \n",
    "        \n",
    "    \n",
    "    accuracy = ((np.array(pred_labels)==np.array(actual_labels)).sum())/np.array(actual_labels).size #size of test set\n",
    "    correct = ((np.array(pred_labels)==np.array(actual_labels)).sum())\n",
    "    total = np.array(actual_labels).size\n",
    "    \n",
    "    \n",
    "    return running_loss, accuracy, correct, total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f203fbb",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1d563e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franceskoback/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 10.60793685913086\n",
      "train loss 40.53252601623535\n",
      "train loss 30.5295701677149\n",
      "train loss 35.41282321512699\n",
      "train loss 31.946093025661654\n",
      "train loss 30.82361612870143\n",
      "train loss 32.510853267485096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franceskoback/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg Training loss: 31.430\n",
      "Epoch 1 avg Valid loss: 31.409\n",
      "Epoch 1 Valid accuracy: 7.0% (7 of 10 right)\n",
      "\n",
      "train loss 44.95121765136719\n",
      "train loss 35.008849779764816\n",
      "train loss 32.57229449532249\n",
      "train loss 33.6308827996254\n",
      "train loss 33.07962376730782\n",
      "train loss 32.16398428036616\n",
      "train loss 32.17995156011273\n",
      "Epoch 2 avg Training loss: 32.952\n",
      "Epoch 2 avg Valid loss: 31.827\n",
      "Epoch 2 Valid accuracy: 7.0% (7 of 10 right)\n",
      "\n",
      "train loss 76.69700622558594\n",
      "train loss 37.7627002398173\n",
      "train loss 32.31963539123535\n",
      "train loss 34.810983300209045\n",
      "train loss 33.83393788337707\n",
      "train loss 30.850416889557465\n",
      "train loss 30.734654003574\n",
      "Epoch 3 avg Training loss: 32.940\n",
      "Epoch 3 avg Valid loss: 31.401\n",
      "Epoch 3 Valid accuracy: 7.0% (7 of 10 right)\n",
      "\n",
      "train loss 37.65026092529297\n",
      "train loss 31.745958646138508\n",
      "train loss 30.26412864164872\n",
      "train loss 31.627083897590634\n",
      "train loss 32.8355773062933\n",
      "train loss 32.766398888367874\n",
      "train loss 32.200712896162464\n",
      "Epoch 4 avg Training loss: 31.414\n",
      "Epoch 4 avg Valid loss: 31.402\n",
      "Epoch 4 Valid accuracy: 17.0% (17 of 10 right)\n",
      "\n",
      "train loss 22.91887855529785\n",
      "train loss 34.51195208231608\n",
      "train loss 30.234799991954457\n",
      "train loss 30.700415849685672\n",
      "train loss 34.10879534766788\n",
      "train loss 32.63524708381066\n",
      "train loss 32.154410223807055\n",
      "Epoch 5 avg Training loss: 31.348\n",
      "Epoch 5 avg Valid loss: 32.308\n",
      "Epoch 5 Valid accuracy: 17.0% (17 of 10 right)\n",
      "\n",
      "train loss 6.1309685707092285\n",
      "train loss 39.9185327688853\n",
      "train loss 35.27878444845027\n",
      "train loss 29.93749925494194\n",
      "train loss 29.66427907489595\n",
      "train loss 31.537000876206616\n",
      "train loss 30.567351402774936\n",
      "Epoch 6 avg Training loss: 31.828\n",
      "Epoch 6 avg Valid loss: 31.418\n",
      "Epoch 6 Valid accuracy: 17.0% (17 of 10 right)\n",
      "\n",
      "train loss 52.59606170654297\n",
      "train loss 34.31816343466441\n",
      "train loss 36.34154001149264\n",
      "train loss 34.53398394584656\n",
      "train loss 33.7671138218471\n",
      "train loss 32.72506405757024\n",
      "train loss 32.0728864362163\n",
      "Epoch 7 avg Training loss: 31.404\n",
      "Epoch 7 avg Valid loss: 31.834\n",
      "Epoch 7 Valid accuracy: 17.0% (17 of 10 right)\n",
      "\n",
      "train loss 8.609097480773926\n",
      "train loss 27.966333230336506\n",
      "train loss 28.584011858159847\n",
      "train loss 31.112462759017944\n",
      "train loss 31.05939326967512\n",
      "train loss 31.257342100143436\n",
      "train loss 31.404215366609638\n",
      "Epoch 8 avg Training loss: 32.970\n",
      "Epoch 8 avg Valid loss: 32.941\n",
      "Epoch 8 Valid accuracy: 19.0% (19 of 10 right)\n",
      "\n",
      "train loss 34.680076599121094\n",
      "train loss 23.166428565979004\n",
      "train loss 27.51040558381514\n",
      "train loss 27.489468932151794\n",
      "train loss 27.2868215470087\n",
      "train loss 29.820931287912224\n",
      "train loss 32.7604794040803\n",
      "Epoch 9 avg Training loss: 31.649\n",
      "Epoch 9 avg Valid loss: 32.916\n",
      "Epoch 9 Valid accuracy: 7.0% (7 of 10 right)\n",
      "\n",
      "train loss 62.97537612915039\n",
      "train loss 55.77116362253825\n",
      "train loss 41.1902852491899\n",
      "train loss 39.094174265861504\n",
      "train loss 36.581444604056216\n",
      "train loss 35.35372383777912\n",
      "train loss 32.532571438820135\n",
      "Epoch 10 avg Training loss: 31.836\n",
      "Epoch 10 avg Valid loss: 32.053\n",
      "Epoch 10 Valid accuracy: 7.0% (7 of 10 right)\n",
      "\n",
      "train loss 5.08427619934082\n",
      "train loss 19.562360445658367\n",
      "train loss 22.921679063276812\n",
      "train loss 25.990459382534024\n",
      "train loss 28.370427449544266\n",
      "train loss 29.76615304213303\n",
      "train loss 31.20970652180333\n",
      "Epoch 11 avg Training loss: 31.413\n",
      "Epoch 11 avg Valid loss: 31.407\n",
      "Epoch 11 Valid accuracy: 17.0% (17 of 10 right)\n",
      "\n",
      "train loss 25.922367095947266\n",
      "train loss 35.98276011149088\n",
      "train loss 31.818323395468973\n",
      "train loss 33.60469210147858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m valid_losses \u001b[39m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(params[\u001b[39m'\u001b[39m\u001b[39mn_epochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m----> 5\u001b[0m     train_loss \u001b[39m=\u001b[39m train_one_epoch(epoch, model, loss_fn, optimizer, train_loader)\n\u001b[1;32m      6\u001b[0m     train_losses\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [train_loss]\n\u001b[1;32m      7\u001b[0m     valid_loss, accuracy, correct, total \u001b[39m=\u001b[39m test_one_epoch(epoch, model, loss_fn, valid_loader, len_val)\n",
      "Cell \u001b[0;32mIn [7], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch, model, loss_fn, optimizer, train_loader, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m---> 20\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# sets all grads to None \u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tensorflow/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(params['n_epochs']):\n",
    "    train_loss = train_one_epoch(epoch, model, loss_fn, optimizer, train_loader)\n",
    "    train_losses+= [train_loss]\n",
    "    valid_loss, accuracy, correct, total = test_one_epoch(epoch, model, loss_fn, valid_loader, len_val)\n",
    "    valid_losses+=[valid_loss]\n",
    "    print('Epoch {} avg Valid loss: {:.3f}'.format(epoch+1, valid_loss))\n",
    "    print('Epoch {} Valid accuracy: {:.1%} ({} of {} right)\\n'.format(epoch+1, accuracy, correct, total))\n",
    "    if len(valid_losses)>1 and (valid_loss<min(valid_losses[:-1])):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_fn,\n",
    "            }, 'checkpoint.tar')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e920b1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 31.823\n",
      "Test accuracy: 70.0% (7 of 10 right)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "checkpoint = torch.load('checkpoint.tar')\n",
    "loaded_model = Net(params['num_classes'])\n",
    "loaded_model.to(params[\"device\"])\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "loaded_criterion = checkpoint['loss']\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr = 0.003)\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "last_epoch = checkpoint['epoch']+1\n",
    "\n",
    "test_loss, accuracy, correct, total = test_one_epoch(None, loaded_model, loaded_criterion, test_loader, len_val)\n",
    "\n",
    "print('Test loss: {:.3f}'.format(test_loss))\n",
    "print('Test accuracy: {:.1%} ({} of {} right)\\n'.format(accuracy, correct, total))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d2f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66cfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
